apiVersion: v1
kind: ConfigMap
metadata:
  name: aegra-config-files
  namespace: aegra
  labels:
    app.kubernetes.io/name: aegra
    app.kubernetes.io/part-of: aegra
data:
  # aegra.json - Main configuration file
  aegra.json: |
    {
      "graphs": {
        "legal_agent": "./graphs/legal_agent/graph.py:graph",
        "agent": "./graphs/react_agent/graph.py:graph",
        "agent_hitl": "./graphs/react_agent_hitl/graph.py:graph",
        "subgraph_agent": "./graphs/subgraph_agent/graph.py:graph"
        }
    }
  
  # Graph files
  graphs_legal_agent___init___py: |
    # Legal agent module
    pass

  graphs_legal_agent_graph_py: |
    """legal-agent main module

    This module wires up the core agent and tools:

    - `nl2sql_tool`: Safely converts natural language questions into SQLite `SELECT` queries
      against `data/legal.db`, executing them and returning a tabular preview of results.
    - `rag_search_tool`: Performs semantic retrieval against a Chroma vector store persisted in
      `data/chroma/legal`, supporting metadata filters on `created_utc` and `text_label`.

    The ReAct-style agent is instantiated with these tools via `create_react_agent` using
    the `openai:gpt-4.1` chat model configured by environment variables.

    Environment:
    - Requires OpenAI credentials (e.g., `OPENAI_API_KEY`) loaded via `.env`.

    """

    from typing import Annotated, List, Tuple

    from typing_extensions import TypedDict

    from langgraph.graph import StateGraph, START, END
    from langgraph.graph.message import add_messages

    import sqlite3
    from pathlib import Path
    from collections import Counter
    from langchain.chat_models import init_chat_model
    from langchain_core.tools import tool
    from langgraph.prebuilt import create_react_agent
    from dotenv import load_dotenv
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    from langfuse.langchain import CallbackHandler
    import chromadb
    from chromadb.config import Settings

    load_dotenv()

    BASE_DIR = Path(__file__).resolve().parent
    DATA_PATH = BASE_DIR / "data"
    # langfuse_handler = CallbackHandler()

    def _introspect_schema(conn: sqlite3.Connection) -> Tuple[str, List[str]]:
        """Return a printable schema summary and list of table names for a SQLite DB."""
        cur = conn.cursor()
        tables = [
            row[0]
            for row in cur.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name;"
            ).fetchall()
        ]
        schema_lines: List[str] = []
        for t in tables:
            cols = cur.execute(f"PRAGMA table_info({t});").fetchall()
            col_defs = ", ".join([f"{c[1]} {c[2]}" for c in cols])
            schema_lines.append(f"TABLE {t}({col_defs})")
        return "\n".join(schema_lines), tables


    def _ensure_select_only(sql: str) -> str:
        """Normalize an LLM-produced SQL string and assert it is a SELECT.

        Strips code fences and `sql` language tags if present, enforces the statement
        starts with `SELECT`, and removes a trailing semicolon.
        """
        s = sql.strip().strip(";")
        if s.startswith("```"):
            parts = s.split("```")
            if len(parts) >= 3:
                s = parts[1] if not parts[0] else parts[1]
                s = s.strip()
        if s.lower().startswith("sql\n"):
            s = s[4:].strip()
        if not s.lower().lstrip().startswith("select"):
            raise ValueError("Generated SQL is not a SELECT statement.")
        return s


    @tool("nl2sql")
    def nl2sql_tool(question: str, db_path: str = str(DATA_PATH / "legal.db"), limit: int = 50) -> str:
        """Translate NL to a safe SQLite SELECT and preview results.

        Dataset and schema:
        - Table: `posts(id, created_utc, full_link, title, body, text_label, flair_label)`.
        - Typical lexical targets: `title`, `body`. Typical filters: `created_utc`, `text_label`.

        How this helps users and the LLM for lexical search:
        - Allows keyword-based lookups over `title`/`body` via SQL (e.g., `LOWER(title) LIKE '%eviction%'`).
        - Supports time windows using epoch seconds on `created_utc` (e.g., 2019 → 1546300800..1577836799).
        - Supports topical filtering using `text_label` (single label or `IN (...)`).
        - Ideal for counts, aggregates, top-N, and precise tabular answers grounded in the relational data.

        When to use vs. semantic RAG (`rag_search`):
        - Use `nl2sql` for exact numbers, lists, or filters by keywords/labels/dates.
        - Use `rag_search` for open-ended synthesis or when you want relevant passages by meaning.

        Tips for lexical queries the LLM can generate:
        - Use `LOWER(title) LIKE '%term%' OR LOWER(body) LIKE '%term%'` for case-insensitive keyword search.
        - Combine with `created_utc BETWEEN <start_epoch> AND <end_epoch>` for date ranges.
        - Add `AND text_label = 'housing'` or `AND text_label IN ('housing','contract')` for topical focus.

        Parameters:
        - question: Natural language query to convert to SQL.
        - db_path: Path to the SQLite database file.
        - limit: Row cap for the results preview and a default LIMIT if absent.
        """
        local_llm = init_chat_model("openai:gpt-4o-mini")
        conn = sqlite3.connect(db_path)
        try:
            schema_text, _ = _introspect_schema(conn)
            sys_msg = (
                "You are a careful SQLite expert. Given a user question and the database schema, "
                "produce exactly ONE SQLite SELECT query that answers it. Use only available tables/columns. "
                f"If no LIMIT appears, append LIMIT {limit}. Do not include commentary."
            )
            user_msg = (
                f"SCHEMA:\n{schema_text}\n\n"
                f"QUESTION: {question}\n\n"
                "Return only the SQL."
            )
            resp = local_llm.invoke([
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": user_msg},
            ])
            sql = _ensure_select_only(resp.content)

            cur = conn.cursor()
            try:
                rows = cur.execute(sql).fetchmany(limit)
                colnames = [d[0] for d in cur.description] if cur.description else []
            except Exception as e:
                return f"SQL:\n{sql}\n\nERROR: {e}"

            lines = []
            if colnames:
                lines.append("\t".join(colnames))
            for r in rows:
                lines.append("\t".join(["" if v is None else str(v) for v in r]))
            result_text = "\n".join(lines)
            return f"SQL:\n{sql}\n\nRESULTS (up to {limit} rows):\n{result_text}"
        finally:
            conn.close()


    @tool("rag_search")
    def rag_search_tool(
        query: str,
        top_k: int = 5,
        persist_dir: str = str(DATA_PATH / "chroma" / "legal"),
        collection: str = "legal_posts",
        embedding_model: str = "text-embedding-3-small",
        created_utc_gte: int | None = None,
        created_utc_lte: int | None = None,
        text_label: str | List[str] | None = None,
    ) -> str:
        """Semantic RAG over the legal posts corpus with metadata filters.

        Corpus and fields:
        - Source: `datasets/legal-dataset.jsonl` loaded into Chroma at `data/chroma/legal`.
        - Content: `page_content` is the concatenation of `title` + two newlines + `body`.
        - Metadata: `created_utc` (Unix epoch seconds), `text_label` (string category).

        What this tool helps with:
        - Retrieves semantically similar posts for open-ended questions (e.g., "trends in housing disputes").
        - Narrows results by time window using `created_utc_gte`/`created_utc_lte`.
        - Focuses on topical slices via `text_label` (single label or list using `$in`).
        - Enables the LLM to ground summaries/answers in concrete retrieved evidence.

        When to use vs. `nl2sql`:
        - Use `rag_search` for unstructured, qualitative, or example-driven queries; when you want passages to quote or summarize; or when you don't know precise columns.
        - Use `nl2sql` for exact counts, aggregations, or tabular lookups over the relational `data/legal.db`.

        Parameters:
        - query: Free-text query to search for.
        - top_k: Number of results to return.
        - persist_dir: Directory containing the Chroma DB.
        - collection: Collection name inside Chroma.
        - embedding_model: OpenAI embedding model name.
        - created_utc_gte: Only include docs with `created_utc` >= this value (e.g., 2019-01-01 00:00:00 UTC → 1546300800).
        - created_utc_lte: Only include docs with `created_utc` <= this value (e.g., 2019-12-31 23:59:59 UTC → 1577836799).
        - text_label: Single label (string) or list of labels (uses `$in`).

        Filter semantics:
        - Multiple field conditions are combined under a top-level `$and`.
        - Ranges are expressed as two separate operator expressions: `{created_utc: {$gte: ...}}` and `{created_utc: {$lte: ...}}`.
        """
        embeddings = OpenAIEmbeddings(model=embedding_model)
        
        # Ensure the persist directory exists and is writable
        persist_path = Path(persist_dir)
        persist_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize ChromaDB client with settings to allow proper access
        chroma_client = chromadb.PersistentClient(
            path=str(persist_path),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True,
            ),
        )
        
        vectorstore = Chroma(
            client=chroma_client,
            collection_name=collection,
            embedding_function=embeddings,
        )

        # Build Chroma-compatible filter for metadata fields.
        # Notes for future readers:
        # - Chroma expects each field's operator expression to contain exactly one operator.
        #   Therefore, for a range we MUST split into two separate clauses:
        #     {"created_utc": {"$gte": ...}} and {"created_utc": {"$lte": ...}}
        # - When multiple field conditions are present, they must be wrapped under a
        #   single top-level logical operator. We use "$and" here to intersect filters.
        clauses: List[dict] = []
        # Split range into two separate operator expressions to satisfy Chroma's filter grammar
        if created_utc_gte is not None:
            clauses.append({"created_utc": {"$gte": int(created_utc_gte)}})
        if created_utc_lte is not None:
            clauses.append({"created_utc": {"$lte": int(created_utc_lte)}})
        if text_label is not None:
            if isinstance(text_label, list):
                clauses.append({"text_label": {"$in": text_label}})
            else:
                clauses.append({"text_label": text_label})

        # Collapse the clauses into the final "where" filter Chroma expects.
        # - 0 clauses → no filter (None)
        # - 1 clause  → pass the single dict directly
        # - >1 clauses → wrap with top-level "$and"
        where = None
        if len(clauses) == 1:
            where = clauses[0]
        elif len(clauses) > 1:
            where = {"$and": clauses}

        # Perform semantic search with optional metadata filter.
        # Prefer the API that returns (Document, relevance_score) pairs. If that
        # method is not available in the environment, fall back to plain similarity
        # search and synthesize a (doc, None) tuple for a consistent downstream shape.
        try:
            results = vectorstore.similarity_search_with_relevance_scores(
                query, k=top_k, filter=where
            )
        except Exception:
            # Fallback if relevance scores API is unavailable
            docs = vectorstore.similarity_search(query, k=top_k, filter=where)
            results = [(d, None) for d in docs]

        # Format a compact, readable preview:
        # - Header shows k, applied filters, and collection
        # - Each entry prints score (if available), key metadata, and a truncated snippet
        lines: List[str] = []
        lines.append(
            f"RESULTS (k={top_k}, filters={where if where is not None else {}}, collection='{collection}')"
        )
        for idx, (doc, score) in enumerate(results, start=1):
            meta = doc.metadata or {}
            created = meta.get("created_utc")
            label = meta.get("text_label")
            snippet = (doc.page_content or "").strip()
            # Truncate long content to keep terminal output skimmable
            if len(snippet) > 600:
                snippet = snippet[:600] + "..."
            lines.append(
                f"{idx}. score={score if score is not None else 'n/a'} | created_utc={created} | text_label={label}\n{snippet}"
            )
        return "\n\n".join(lines)

    llm_agent = init_chat_model("openai:gpt-4.1")
    tools = [nl2sql_tool, rag_search_tool]
    graph = create_react_agent(llm_agent, tools)

    # for s in agent.stream({"messages": [{"role": "user", "content": "What are the top 10 labels by count?"}]},
    # config={"callbacks": [langfuse_handler]}):
    #     print(s)
  
  # auth.py - Authentication module
  auth.py: |
    """
    Authentication configuration for LangGraph Agent Server.

    This module provides environment-based authentication switching between:
    - noop: No authentication (allow all requests)
    - custom: Custom authentication integration

    Set AUTH_TYPE environment variable to choose authentication mode.
    """

    import logging
    import os
    from typing import Any

    from langgraph_sdk import Auth

    logger = logging.getLogger(__name__)

    # Initialize LangGraph Auth instance
    auth = Auth()

    # Get authentication type from environment
    AUTH_TYPE = os.getenv("AUTH_TYPE", "noop").lower()

    if AUTH_TYPE == "noop":
        logger.info("Using noop authentication (no auth required)")

        @auth.authenticate
        async def authenticate(headers: dict[str, str]) -> Auth.types.MinimalUserDict:
            """No-op authentication that allows all requests."""
            _ = headers  # Suppress unused warning
            return {
                "identity": "anonymous",
                "display_name": "Anonymous User",
                "is_authenticated": True,
            }

        @auth.on
        async def authorize(
            ctx: Auth.types.AuthContext, value: dict[str, Any]
        ) -> dict[str, Any]:
            """No-op authorization that allows access to all resources."""
            _ = ctx, value  # Suppress unused warnings
            return {}  # Empty filter = no access restrictions

    elif AUTH_TYPE == "custom":
        logger.info("Using custom authentication")

        @auth.authenticate
        async def authenticate(headers: dict[str, str]) -> Auth.types.MinimalUserDict:
            """
            Custom authentication handler.

            Modify this function to integrate with your authentication service.
            """
            # Extract authorization header
            authorization = (
                headers.get("authorization")
                or headers.get("Authorization")
                or headers.get(b"authorization")
                or headers.get(b"Authorization")
            )

            # Handle bytes headers
            if isinstance(authorization, bytes):
                authorization = authorization.decode("utf-8")

            if not authorization:
                logger.warning("Missing Authorization header")
                raise Auth.exceptions.HTTPException(
                    status_code=401, detail="Authorization header required"
                )

            # Development token for testing
            if authorization == "Bearer dev-token":
                return {
                    "identity": "dev-user",
                    "display_name": "Development User",
                    "email": "dev@example.com",
                    "permissions": ["admin"],
                    "org_id": "dev-org",
                    "is_authenticated": True,
                }

            # Example: Simple API key validation (replace with your logic)
            if authorization.startswith("Bearer "):
                # TODO: Replace with your auth service integration
                logger.warning("Invalid token")
                raise Auth.exceptions.HTTPException(
                    status_code=401, detail="Invalid authentication token"
                )

            # Reject requests without proper format
            raise Auth.exceptions.HTTPException(
                status_code=401,
                detail="Invalid authorization format. Expected 'Bearer <token>'",
            )

        @auth.on
        async def authorize(
            ctx: Auth.types.AuthContext, value: dict[str, Any]
        ) -> dict[str, Any]:
            """
            Multi-tenant authorization with user-scoped access control.
            """
            try:
                # Get user identity from authentication context
                user_id = ctx.user.identity

                if not user_id:
                    logger.error("Missing user identity in auth context")
                    raise Auth.exceptions.HTTPException(
                        status_code=401, detail="Invalid user identity"
                    )

                # Create owner filter for resource access control
                owner_filter = {"owner": user_id}

                # Add owner information to metadata for create/update operations
                metadata = value.setdefault("metadata", {})
                metadata.update(owner_filter)

                # Return filter for database operations
                return owner_filter

            except Auth.exceptions.HTTPException:
                raise
            except Exception as e:
                logger.error(f"Authorization error: {e}", exc_info=True)
                raise Auth.exceptions.HTTPException(
                    status_code=500, detail="Authorization system error"
                ) from e

    else:
        raise ValueError(
            f"Unknown AUTH_TYPE: {AUTH_TYPE}. Supported values: 'noop', 'custom'"
        )
  
  # .env - Environment variables
  .env: |
    # Environment variables for Aegra
    # Add your environment-specific variables here
    DEBUG=true
    AUTH_TYPE=noop
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aegra-alembic-files
  namespace: aegra
  labels:
    app.kubernetes.io/name: aegra
    app.kubernetes.io/part-of: aegra
data:
  # alembic.ini - Database migration configuration
  alembic.ini: |
    # A generic, single database configuration.

    [alembic]
    # path to migration scripts
    script_location = alembic

    # template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
    # Uncomment the line below if you want the files to be prepended with date and time
    # file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

    # sys.path path, will be prepended to sys.path if present.
    # defaults to the current working directory.
    prepend_sys_path = .

    # timezone to use when rendering the date within the migration file
    # as well as the filename.
    # If specified, requires the python-dateutil library that can be
    # installed by adding `alembic[tz]` to the pip requirements
    # string value is passed to dateutil.tz.gettz()
    # leave blank for localtime
    # timezone =

    # max length of characters to apply to the
    # "slug" field
    # truncate_slug_length = 40

    # set to 'true' to run the environment during
    # the 'revision' command, regardless of autogenerate
    # revision_environment = false

    # set to 'true' to allow .pyc and .pyo files without
    # a source .py file to be detected as revisions in the
    # versions/ directory
    # sourceless = false

    # version number format
    version_num_format = %04d

    # version path separator; As mentioned above, this is the character used to split
    # version_locations. The default within new alembic.ini files is "os", which uses
    # os.pathsep. If this key is omitted entirely, it falls back to the legacy
    # behavior of splitting on spaces and/or commas.
    # Valid values for version_path_separator are:
    #
    # version_path_separator = :
    # version_path_separator = ;
    # version_path_separator = space
    version_path_separator = os

    # the output encoding used when revision files
    # are written from script.py.mako
    # output_encoding = utf-8

    sqlalchemy.url = postgresql+asyncpg://user:password@postgres-service:5432/aegra


    [post_write_hooks]
    # post_write_hooks defines scripts or Python functions that are run
    # on newly generated revision scripts.  See the documentation for further
    # detail and examples

    # format using "black" - use the console_scripts runner, against the "black" entrypoint
    # hooks = black
    # black.type = console_scripts
    # black.entrypoint = black
    # black.options = -l 79 REVISION_SCRIPT_FILENAME

    # Logging configuration
    [loggers]
    keys = root,sqlalchemy,alembic

    [handlers]
    keys = console

    [formatters]
    keys = generic

    [logger_root]
    level = WARN
    handlers = console
    qualname =

    [logger_sqlalchemy]
    level = WARN
    handlers =
    qualname = sqlalchemy.engine

    [logger_alembic]
    level = INFO
    handlers =
    qualname = alembic

    [handler_console]
    class = StreamHandler
    args = (sys.stderr,)
    level = NOTSET
    formatter = generic

    [formatter_generic]
    format = %(levelname)-5.5s [%(name)s] %(message)s
    datefmt = %H:%M:%S

  # env.py - Alembic environment file
  env.py: |
    """Alembic environment configuration for Aegra database migrations."""

    import asyncio
    import os
    from logging.config import fileConfig

    from dotenv import load_dotenv
    from sqlalchemy import pool
    from sqlalchemy.engine import Connection
    from sqlalchemy.ext.asyncio import async_engine_from_config

    from alembic import context

    # Import your SQLAlchemy models here
    from src.agent_server.core.orm import Base

    # Load environment variables from a .env file if present
    load_dotenv()

    # This is the Alembic Config object, which provides
    # access to the values within the .ini file in use.
    config = context.config

    # Interpret the config file for Python logging.
    # This line sets up loggers basically.
    if config.config_file_name is not None:
        fileConfig(config.config_file_name)

    # add your model's MetaData object here
    # for 'autogenerate' support
    target_metadata = Base.metadata

    # other values from the config, defined by the needs of env.py,
    # can be acquired:
    # my_important_option = config.get_main_option("my_important_option")
    # ... etc.


    def get_url():
        """Get database URL from environment or config."""
        return os.getenv("DATABASE_URL", config.get_main_option("sqlalchemy.url"))


    def run_migrations_offline() -> None:
        """Run migrations in 'offline' mode.

        This configures the context with just a URL
        and not an Engine, though an Engine is acceptable
        here as well.  By skipping the Engine creation
        we don't even need a DBAPI to be available.

        Calls to context.execute() here emit the given string to the
        script output.

        """
        url = get_url()
        context.configure(
            url=url,
            target_metadata=target_metadata,
            literal_binds=True,
            dialect_opts={"paramstyle": "named"},
        )

        with context.begin_transaction():
            context.run_migrations()


    def do_run_migrations(connection: Connection) -> None:
        """Run migrations with the given connection."""
        context.configure(connection=connection, target_metadata=target_metadata)

        with context.begin_transaction():
            context.run_migrations()


    async def run_async_migrations() -> None:
        """In this scenario we need to create an Engine
        and associate a connection with the context.

        """
        configuration = config.get_section(config.config_ini_section)
        configuration["sqlalchemy.url"] = get_url()

        connectable = async_engine_from_config(
            configuration,
            prefix="sqlalchemy.",
            poolclass=pool.NullPool,
        )

        async with connectable.connect() as connection:
            await connection.run_sync(do_run_migrations)

        await connectable.dispose()


    def run_migrations_online() -> None:
        """Run migrations in 'online' mode."""
        asyncio.run(run_async_migrations())


    if context.is_offline_mode():
        run_migrations_offline()
    else:
        run_migrations_online()

  # script.py.mako - Alembic migration template
  script.py.mako: |
    """${message}

    Revision ID: ${up_revision}
    Revises: ${down_revision | comma,n}
    Create Date: ${create_date}

    """
    from alembic import op
    import sqlalchemy as sa
    ${imports if imports else ""}

    # revision identifiers, used by Alembic.
    revision = ${repr(up_revision)}
    down_revision = ${repr(down_revision)}
    branch_labels = ${repr(branch_labels)}
    depends_on = ${repr(depends_on)}


    def upgrade() -> None:
        ${upgrades if upgrades else "pass"}


    def downgrade() -> None:
        ${downgrades if downgrades else "pass"}

  # Migration files - versions directory
  versions_20250817172544_initial_schema.py: |
    """Initial database schema for Aegra Agent Protocol server

    This migration creates the core tables for the Agent Protocol implementation:
    - assistant: Stores assistant configurations and metadata
    - thread: Manages conversation threads with status tracking
    - runs: Tracks execution runs with input/output and status
    - run_events: Stores streaming events for real-time communication

    Revision ID: 7b79bfd12626
    Revises:
    Create Date: 2025-08-17 17:25:44.338823

    """

    import sqlalchemy as sa
    from sqlalchemy.dialects import postgresql

    from alembic import op

    # revision identifiers, used by Alembic.
    revision = "7b79bfd12626"
    down_revision = None
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        """Create initial database schema for Aegra Agent Protocol server."""

        # Create PostgreSQL extensions
        op.execute('CREATE EXTENSION IF NOT EXISTS "uuid-ossp";')

        # Create assistant table
        op.create_table(
            "assistant",
            sa.Column(
                "assistant_id",
                sa.Text(),
                server_default=sa.text("uuid_generate_v4()::text"),
                nullable=False,
            ),
            sa.Column("name", sa.Text(), nullable=False),
            sa.Column("description", sa.Text(), nullable=True),
            sa.Column("graph_id", sa.Text(), nullable=False),
            sa.Column(
                "config",
                postgresql.JSONB(astext_type=sa.Text()),
                server_default=sa.text("'{}'::jsonb"),
                nullable=False,
            ),
            sa.Column("user_id", sa.Text(), nullable=False),
            sa.Column(
                "created_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.Column(
                "updated_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.PrimaryKeyConstraint("assistant_id"),
        )

        # Create run_events table for streaming functionality
        op.create_table(
            "run_events",
            sa.Column("id", sa.Text(), nullable=False),
            sa.Column("run_id", sa.Text(), nullable=False),
            sa.Column("seq", sa.Integer(), nullable=False),
            sa.Column("event", sa.Text(), nullable=False),
            sa.Column("data", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
            sa.Column(
                "created_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.PrimaryKeyConstraint("id"),
        )

        # Create thread table
        op.create_table(
            "thread",
            sa.Column("thread_id", sa.Text(), nullable=False),
            sa.Column(
                "status", sa.Text(), server_default=sa.text("'idle'"), nullable=False
            ),
            sa.Column(
                "metadata_json",
                postgresql.JSONB(astext_type=sa.Text()),
                server_default=sa.text("'{}'::jsonb"),
                nullable=False,
            ),
            sa.Column("user_id", sa.Text(), nullable=False),
            sa.Column(
                "created_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.Column(
                "updated_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.PrimaryKeyConstraint("thread_id"),
        )

        # Create runs table with foreign key constraints
        op.create_table(
            "runs",
            sa.Column(
                "run_id",
                sa.Text(),
                server_default=sa.text("uuid_generate_v4()::text"),
                nullable=False,
            ),
            sa.Column("thread_id", sa.Text(), nullable=False),
            sa.Column("assistant_id", sa.Text(), nullable=True),
            sa.Column(
                "status", sa.Text(), server_default=sa.text("'pending'"), nullable=False
            ),
            sa.Column(
                "input",
                postgresql.JSONB(astext_type=sa.Text()),
                server_default=sa.text("'{}'::jsonb"),
                nullable=True,
            ),
            sa.Column("config", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
            sa.Column("output", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
            sa.Column("error_message", sa.Text(), nullable=True),
            sa.Column("user_id", sa.Text(), nullable=False),
            sa.Column(
                "created_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.Column(
                "updated_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.ForeignKeyConstraint(["assistant_id"], ["assistant.assistant_id"]),
            sa.ForeignKeyConstraint(["thread_id"], ["thread.thread_id"]),
            sa.PrimaryKeyConstraint("run_id"),
        )

        # Create indexes for performance optimization
        # Assistant indexes
        op.create_index("idx_assistant_user", "assistant", ["user_id"])
        op.create_index(
            "idx_assistant_user_graph", "assistant", ["user_id", "graph_id"], unique=True
        )

        # Run events indexes
        op.create_index("idx_run_events_run_id", "run_events", ["run_id"])
        op.create_index("idx_run_events_seq", "run_events", ["run_id", "seq"])

        # Thread indexes
        op.create_index("idx_thread_user", "thread", ["user_id"])

        # Runs indexes
        op.create_index("idx_runs_assistant_id", "runs", ["assistant_id"])
        op.create_index("idx_runs_created_at", "runs", ["created_at"])
        op.create_index("idx_runs_status", "runs", ["status"])
        op.create_index("idx_runs_thread_id", "runs", ["thread_id"])
        op.create_index("idx_runs_user", "runs", ["user_id"])


    def downgrade() -> None:
        """Drop all tables and indexes created in this migration."""

        # Drop indexes in reverse order (respecting dependencies)
        # Runs indexes
        op.drop_index("idx_runs_user", table_name="runs")
        op.drop_index("idx_runs_thread_id", table_name="runs")
        op.drop_index("idx_runs_status", table_name="runs")
        op.drop_index("idx_runs_created_at", table_name="runs")
        op.drop_index("idx_runs_assistant_id", table_name="runs")

        # Thread indexes
        op.drop_index("idx_thread_user", table_name="thread")

        # Run events indexes
        op.drop_index("idx_run_events_seq", table_name="run_events")
        op.drop_index("idx_run_events_run_id", table_name="run_events")

        # Assistant indexes
        op.drop_index("idx_assistant_user_graph", table_name="assistant")
        op.drop_index("idx_assistant_user", table_name="assistant")

        # Drop tables in reverse order (respecting foreign key constraints)
        op.drop_table("runs")
        op.drop_table("thread")
        op.drop_table("run_events")
        op.drop_table("assistant")

  versions_20250830161758_add_context.py: |
    """Add context

    Revision ID: 5931cd77e93b
    Revises: 7b79bfd12626
    Create Date: 2025-08-30 16:17:58.752557

    """

    import sqlalchemy as sa
    from sqlalchemy.dialects import postgresql

    from alembic import op

    # revision identifiers, used by Alembic.
    revision = "5931cd77e93b"
    down_revision = "7b79bfd12626"
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        # ### commands auto generated by Alembic - please adjust! ###
        op.add_column(
            "assistant",
            sa.Column(
                "context",
                postgresql.JSONB(astext_type=sa.Text()),
                server_default=sa.text("'{}'::jsonb"),
                nullable=False,
            ),
        )
        op.add_column(
            "runs",
            sa.Column("context", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        )
        # ### end Alembic commands ###


    def downgrade() -> None:
        # ### commands auto generated by Alembic - please adjust! ###
        op.drop_column("runs", "context")
        op.drop_column("assistant", "context")
        # ### end Alembic commands ###

  versions_20250831174511_add_cascade_delete_for_runs_thread_fkey.py: |
    """add_cascade_delete_for_runs_thread_fkey

    Revision ID: 11b2402d4be1
    Revises: 5931cd77e93b
    Create Date: 2025-08-31 17:45:11.723042

    """

    from alembic import op

    # revision identifiers, used by Alembic.
    revision = "11b2402d4be1"
    down_revision = "5931cd77e93b"
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        """Add ON DELETE CASCADE to runs.thread_id foreign key constraint."""

        # Drop the existing foreign key constraint
        op.drop_constraint("runs_thread_id_fkey", "runs", type_="foreignkey")

        # Recreate the foreign key constraint with CASCADE DELETE
        op.create_foreign_key(
            "runs_thread_id_fkey",
            "runs",
            "thread",
            ["thread_id"],
            ["thread_id"],
            ondelete="CASCADE",
        )


    def downgrade() -> None:
        """Remove CASCADE DELETE from runs.thread_id foreign key constraint."""

        # Drop the CASCADE foreign key constraint
        op.drop_constraint("runs_thread_id_fkey", "runs", type_="foreignkey")

        # Recreate the original foreign key constraint without CASCADE
        op.create_foreign_key(
            "runs_thread_id_fkey", "runs", "thread", ["thread_id"], ["thread_id"]
        )

  versions_20250913193817_add_version_table.py: |
    """Add version table

    Revision ID: 76dfdbe90d2b
    Revises: 11b2402d4be1
    Create Date: 2025-09-13 19:38:17.246644

    """

    import sqlalchemy as sa
    from sqlalchemy.dialects import postgresql

    from alembic import op

    # revision identifiers, used by Alembic.
    revision = "76dfdbe90d2b"
    down_revision = "11b2402d4be1"
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        # ### commands auto generated by Alembic - please adjust! ###
        op.create_table(
            "assistant_versions",
            sa.Column("assistant_id", sa.Text(), nullable=False),
            sa.Column("version", sa.Integer(), nullable=False),
            sa.Column("graph_id", sa.Text(), nullable=False),
            sa.Column("config", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
            sa.Column("context", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
            sa.Column(
                "created_at",
                sa.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                nullable=False,
            ),
            sa.Column(
                "metadata",
                postgresql.JSONB(astext_type=sa.Text()),
                server_default=sa.text("'{}'::jsonb"),
                nullable=False,
            ),
            sa.Column("name", sa.Text(), nullable=True),
            sa.Column("description", sa.Text(), nullable=True),
            sa.ForeignKeyConstraint(
                ["assistant_id"],
                ["assistant.assistant_id"],
            ),
            sa.PrimaryKeyConstraint("assistant_id", "version"),
        )
        op.add_column(
            "assistant",
            sa.Column("version", sa.Integer(), server_default=sa.text("1"), nullable=False),
        )
        op.drop_index(op.f("idx_assistant_user_graph"), table_name="assistant")
        op.create_index(
            "idx_assistant_user_assistant",
            "assistant",
            ["user_id", "assistant_id"],
            unique=True,
        )
        op.create_index(
            "idx_assistant_user_graph_config",
            "assistant",
            ["user_id", "graph_id", "config"],
            unique=True,
        )
        # ### end Alembic commands ###


    def downgrade() -> None:
        # ### commands auto generated by Alembic - please adjust! ###
        op.drop_index("idx_assistant_user_graph_config", table_name="assistant")
        op.drop_index("idx_assistant_user_assistant", table_name="assistant")
        op.create_index(
            op.f("idx_assistant_user_graph"),
            "assistant",
            ["user_id", "graph_id"],
            unique=True,
        )
        op.drop_column("assistant", "version")
        op.drop_table("assistant_versions")
        # ### end Alembic commands ###

  versions_20250913213535_add_metadata_for_assistant_table_and_.py: |
    """Add metadata for assistant table and cascade del for required foreign keys

    Revision ID: aee821a02fc8
    Revises: 76dfdbe90d2b
    Create Date: 2025-09-13 21:35:35.407522

    """

    import sqlalchemy as sa
    from sqlalchemy.dialects import postgresql

    from alembic import op

    # revision identifiers, used by Alembic.
    revision = "aee821a02fc8"
    down_revision = "76dfdbe90d2b"
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        # ### commands auto generated by Alembic - please adjust! ###
        op.add_column(
            "assistant",
            sa.Column(
                "metadata",
                postgresql.JSONB(astext_type=sa.Text()),
                server_default=sa.text("'{}'::jsonb"),
                nullable=False,
            ),
        )
        op.drop_constraint(
            op.f("assistant_versions_assistant_id_fkey"),
            "assistant_versions",
            type_="foreignkey",
        )
        op.create_foreign_key(
            None,
            "assistant_versions",
            "assistant",
            ["assistant_id"],
            ["assistant_id"],
            ondelete="CASCADE",
        )
        op.drop_constraint(op.f("runs_assistant_id_fkey"), "runs", type_="foreignkey")
        op.create_foreign_key(
            None,
            "runs",
            "assistant",
            ["assistant_id"],
            ["assistant_id"],
            ondelete="CASCADE",
        )
        # ### end Alembic commands ###


    def downgrade() -> None:
        # ### commands auto generated by Alembic - please adjust! ###
        op.drop_constraint(None, "runs", type_="foreignkey")
        op.create_foreign_key(
            op.f("runs_assistant_id_fkey"),
            "runs",
            "assistant",
            ["assistant_id"],
            ["assistant_id"],
        )
        op.drop_constraint(None, "assistant_versions", type_="foreignkey")
        op.create_foreign_key(
            op.f("assistant_versions_assistant_id_fkey"),
            "assistant_versions",
            "assistant",
            ["assistant_id"],
            ["assistant_id"],
        )
        op.drop_column("assistant", "metadata")
        # ### end Alembic commands ###
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aegra-src-files
  namespace: aegra
  labels:
    app.kubernetes.io/name: aegra
    app.kubernetes.io/part-of: aegra
data:
  # This ConfigMap will be populated with the source code files
  # In a real deployment, you would create this from your source directory
  # For now, we'll create a placeholder
  __init__.py: |
    # Placeholder for src/__init__.py
    pass
